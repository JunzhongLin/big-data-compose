version: '3.3'

services:
  postgresql:
    image: docker.io/bitnami/postgresql:10
    volumes:
      - 'postgresql_data:/bitnami/postgresql'
    environment:
      - POSTGRESQL_DATABASE=bitnami_airflow
      - POSTGRESQL_USERNAME=bn_airflow
      - POSTGRESQL_PASSWORD=bitnami1
      # ALLOW_EMPTY_PASSWORD is recommended only for development.
      - ALLOW_EMPTY_PASSWORD=yes

  airflow-scheduler:
    image: docker.io/bitnami/airflow-scheduler:2
    environment:
      - AIRFLOW_SECRET_KEY=LYs5mB40wBD5UoUOj_0b7sWpH4wlYzGDGx3XwHAfKgY=
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=LocalExecutor
      - AIRFLOW_WEBSERVER_HOST=airflow
    volumes:
      - ./dags:/opt/bitnami/airflow/dags
      - ./requirements.txt:/bitnami/python/requirements.txt

  airflow:
    image: docker.io/bitnami/airflow:2
    environment:
      - AIRFLOW_SECRET_KEY=LYs5mB40wBD5UoUOj_0b7sWpH4wlYzGDGx3XwHAfKgY=
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=LocalExecutor
      - AIRFLOW_PASSWORD=bitnami123
      - AIRFLOW_USERNAME=user
    ports:
      - '8080:8080'
    volumes:
      - ./dags:/opt/bitnami/airflow/dags
      - ./requirements.txt:/bitnami/python/requirements.txt
  
  spark-master:
    image: standalone-pyspark:2.3.2-hadoop2.7-py3.7
    ports:
      - "9090:8080"
      - "7077:7077"
      - "4040:4040"
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
  spark-worker-a:
    image: standalone-pyspark:2.3.2-hadoop2.7-py3.7
    ports:
      - "9091:8080"
      - "7000:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a

  spark-worker-b:
    image: standalone-pyspark:2.3.2-hadoop2.7-py3.7
    ports:
      - "9092:8080"
      - "7001:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-b

  livy:
    image: john/livy-spark:0.3.0
    
    ports:
      - "8998:8998"
    depends_on:
      - spark-master
      - spark-worker-a
      - spark-worker-b
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_DEPLOY_MODE=client
      - LOCAL_DIR_WHITELIST=/job

    volumes:
     - ./test_case:/job

volumes:
  postgresql_data:
    driver: local

