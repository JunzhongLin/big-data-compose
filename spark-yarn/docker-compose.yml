version: "3.7"
services:
  hivemetastore:
    image: postgres:11.5
    hostname: hivemetastore
    environment:
      POSTGRES_PASSWORD: new_password
    expose:
      - 5432
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  master:
    image: test/spark-yarn-dep:0.1
#    build: '../hadoop-hive-spark-docker'
    hostname: master
    depends_on:
      - hivemetastore
    environment:
      HADOOP_NODE: namenode
      HIVE_CONFIGURE: yes, please
#      SPARK_PUBLIC_DNS: localhost
#      SPARK_LOCAL_IP: 172.28.1.1
#      SPARK_MASTER_HOST: 172.28.1.1
      SPARK_LOCAL_HOSTNAME: master
    expose:
      - 1-65535
    ports:
      # Spark Master Web UI
      - 8080:8080
      # Spark job Web UI: increments for each successive job
      - 4040:4040
      - 4041:4041
      - 4042:4042
      - 4043:4043
      # Spark History server
      - 18080:18080
      # YARN UI
      - 8088:8088
      # Hadoop namenode UI
      - 9870:9870
      # Hadoop secondary namenode UI
      - 9868:9868
      # Hive JDBC
      - 10000:10000
    volumes:
      - ./data:/data

  worker1:
    image: test/spark-yarn-dep:0.1
#    build: '../hadoop-hive-spark-docker'
    hostname: worker1
    # command: tail -f /dev/null
    depends_on:
      - hivemetastore
    environment:
      SPARK_MASTER_ADDRESS: spark://master:7077
      SPARK_WORKER_PORT: 7000
      SPARK_WORKER_WEBUI_PORT: 8080
#      SPARK_PUBLIC_DNS: localhost
#      SPARK_LOCAL_HOSTNAME: worker1
#      SPARK_LOCAL_IP: 172.28.1.2
#      SPARK_MASTER_HOST: 172.28.1.1
      HADOOP_NODE: datanode
    expose:
      - 1-65535
    ports:
      # Hadoop datanode UI
      - 9864:9864
      #Spark worker UI
      - 8081:8080
      # worker port
      - 7001:7000
    volumes:
      - ./data:/data

#  worker2:
#    image: test/spark-yarn-dep:0.1
##    build: '../hadoop-hive-spark-docker'
#    hostname: worker2
#    depends_on:
#      - hivemetastore
#    command: tail -f /dev/null
#    environment:
#      SPARK_MASTER_ADDRESS: spark://master:7077
#      SPARK_WORKER_PORT: 7000
#      SPARK_WORKER_WEBUI_PORT: 8080
#      SPARK_PUBLIC_DNS: localhost
#      SPARK_LOCAL_HOSTNAME: worker2
#      SPARK_LOCAL_IP: 172.28.1.3
#      SPARK_MASTER_HOST: 172.28.1.1
#      HADOOP_NODE: datanode
#      HADOOP_DATANODE_UI_PORT: 9865
#    expose:
#      - 1-65535
#    ports:
#      # Hadoop datanode UI
#      - 9865:9865
#      # Spark worker UI
#      - 8082:8080
#      # worker port
#      - 7002:7000
#    volumes:
#      - ./data:/data

